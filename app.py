# e:\project\python\pythonpachong\news_crawler\app.py
import streamlit as st
import pandas as pd
import os

# Import modules
from modules.config import TARGET_URL, STORAGE_FILE
from modules.network import get_rss
from modules.parser import parse_rss
from modules.storage import save_to_excel
from modules.ai_reporter import fetch_ai_briefing

# --- Page Config ---
st.set_page_config(
    page_title="SSPai News Crawler",
    page_icon="ğŸ“°",
    layout="centered"
)

# --- Title ---
st.title("ğŸ“° RSS çˆ¬è™« & AI å…³é”®è¯æå–")

# --- Target URL Input ---
target_url_input = st.text_input(
    "ç›®æ ‡ RSS åœ°å€ (Target URL)",
    value=TARGET_URL,
    help="è¾“å…¥éœ€è¦æŠ“å–çš„ RSS è®¢é˜…æºåœ°å€"
)

# --- API Key & Base URL Configuration ---
# Always show sidebar for configuration
with st.sidebar:
    st.header("âš™ï¸ é…ç½®")
    
    # API Key Input
    api_key_input = st.text_input(
        "OpenAI API Key",
        type="password",
        help="è¯·è¾“å…¥æ‚¨çš„ OpenAI API Key",
        key="api_key_input"
    )
    
    # Base URL Input
    from modules.config import AI_CONFIG
    default_base_url = AI_CONFIG.get("base_url", "https://api.openai.com/v1")
    base_url_input = st.text_input(
        "OpenAI ä»£ç†åœ°å€ (Base URL)",
        value=default_base_url,
        help="å¦‚éœ€ä½¿ç”¨ä»£ç†æœåŠ¡å™¨ï¼Œè¯·ä¿®æ”¹æ­¤åœ°å€"
    )
    
    # Model Name Input
    default_model = AI_CONFIG.get("model", "gpt-3.5-turbo")
    model_input = st.text_input(
        "OpenAI æ¨¡å‹åç§° (Model)",
        value=default_model,
        help="å¦‚ gpt-3.5-turboã€gpt-4 ç­‰"
    )
    
    st.divider()
    st.caption("ğŸ’¡ æç¤ºï¼šAPI Key æ”¯æŒä»ç¯å¢ƒå˜é‡ `OPENAI_API_KEY` è¯»å–")

# Determine final API Key (Priority: secrets > env > input)
api_key = None
try:
    api_key = st.secrets.get("OPENAI_API_KEY")
except Exception:
    pass

if not api_key:
    api_key = os.getenv("OPENAI_API_KEY")

if not api_key:
    api_key = api_key_input

if not api_key:
    st.sidebar.warning("âš ï¸ è¯·è¾“å…¥ API Key ä»¥å¯ç”¨ AI ç®€æŠ¥åŠŸèƒ½")

# Determine final Base URL
base_url = base_url_input if base_url_input else default_base_url

# Determine final Model
model = model_input if model_input else default_model

# --- Session State Initialization ---
if "news_data" not in st.session_state:
    st.session_state.news_data = None
if "ai_briefing" not in st.session_state:
    st.session_state.ai_briefing = None
if "crawl_completed" not in st.session_state:
    st.session_state.crawl_completed = False

# --- Main UI ---
col1, col2 = st.columns([3, 1])

with col1:
    start_button = st.button("ğŸš€ å¼€å§‹æŠ“å–", type="primary", use_container_width=True)

with col2:
    if st.button("ğŸ”„ é‡ç½®", use_container_width=True):
        st.session_state.news_data = None
        st.session_state.ai_briefing = None
        st.session_state.crawl_completed = False
        st.rerun()

# --- Crawling Logic ---
if start_button and not st.session_state.crawl_completed:
    if not api_key:
        st.error("âŒ è¯·å…ˆåœ¨ä¾§è¾¹æ è¾“å…¥ API Keyï¼")
    else:
        with st.status("ğŸ”„ æ­£åœ¨å¤„ç†...", expanded=True) as status:
            try:
                # Step 1: Fetch RSS
                status.write("ğŸ“¡ æ­£åœ¨è·å– RSS æ•°æ®...")
                feed = get_rss(target_url_input)
                if not feed:
                    raise Exception("RSS æŠ“å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
                status.write("âœ… RSS æ•°æ®è·å–æˆåŠŸ")

                # Step 2: Parse Data
                status.write("ğŸ“ æ­£åœ¨è§£ææ•°æ®...")
                news_list = parse_rss(feed)
                if not news_list:
                    raise Exception("æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„æ–°é—»æ¡ç›®")
                st.session_state.news_data = news_list
                status.write(f"âœ… æˆåŠŸè§£æ {len(news_list)} æ¡æ–°é—»")

                # Step 3: Archive to Excel (Side effect, non-blocking)
                status.write("ğŸ’¾ æ­£åœ¨å­˜æ¡£åˆ° Excel...")
                try:
                    save_to_excel(news_list, STORAGE_FILE)
                    status.write(f"âœ… å·²å­˜æ¡£è‡³ {STORAGE_FILE}")
                except Exception as e:
                    status.write(f"âš ï¸ å­˜æ¡£å¤±è´¥ï¼ˆéè‡´å‘½é”™è¯¯ï¼‰: {e}")

                # Step 4: Generate AI Keywords
                status.write("ğŸ¤– æ­£åœ¨æå– AI å…³é”®è¯...")
                titles = [item["title"] for item in news_list]
                briefing = fetch_ai_briefing(titles, api_key=api_key, base_url=base_url, model=model)
                if not briefing:
                    raise Exception("AI å…³é”®è¯æå–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ API Key å’Œç½‘ç»œè¿æ¥")
                st.session_state.ai_briefing = briefing
                status.write("âœ… AI å…³é”®è¯æå–æˆåŠŸ")

                # Mark as completed
                st.session_state.crawl_completed = True
                status.update(label="âœ… å¤„ç†å®Œæˆï¼", state="complete", expanded=False)

            except Exception as e:
                status.update(label="âŒ å¤„ç†å¤±è´¥", state="error", expanded=True)
                st.error(f"é”™è¯¯è¯¦æƒ…: {e}")

# --- Display Results ---
if st.session_state.ai_briefing:
    st.divider()
    st.subheader("ğŸ”‘ AI å…³é”®è¯")
    st.markdown(st.session_state.ai_briefing)

if st.session_state.news_data:
    st.divider()
    with st.expander("ğŸ“Š æŸ¥çœ‹åŸå§‹æ•°æ®", expanded=False):
        df = pd.DataFrame(st.session_state.news_data)
        st.dataframe(df, use_container_width=True)
        st.caption(f"å…± {len(df)} æ¡æ–°é—»")
